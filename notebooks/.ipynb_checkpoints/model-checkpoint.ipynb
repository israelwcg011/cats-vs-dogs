{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40fd1439-6c8e-4e71-b30c-ee88a4b4a035",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e76f5807-d158-4f01-8cc5-3f03a30a246b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator,load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import PIL\n",
    "from contextlib import redirect_stdout\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b58cf4-1264-4618-aeff-462b1d263ff1",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41671824-8980-4360-a49d-2ed6cb8baa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image parameters\n",
    "img_width = 128\n",
    "img_height = 128\n",
    "img_channels = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb18cd4c-1d0c-4a9f-8fc3-583aa3988544",
   "metadata": {},
   "source": [
    "# Preprocess data to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b99e92a8-b032-44d1-9a38-ad560c335eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file\n",
    "data = pd.read_csv(\"../datasets/images-and-labels.csv\")\n",
    "\n",
    "# replace labels {0: \"cat\", 1: \"dog\"}\n",
    "data[\"label\"] = data[\"label\"].replace({0: \"cat\", 1: \"dog\"})\n",
    "\n",
    "# train validation split\n",
    "df_train, df_validation = train_test_split(data, test_size=0.15, random_state=2)\n",
    "\n",
    "# reset train and validation dataframes indexes\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_validation = df_validation.reset_index(drop=True)\n",
    "\n",
    "# get some metadada from train and validation dataframes\n",
    "df_train_shape = df_train.shape\n",
    "df_validation_shape = df_validation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ef3453-bb54-4725-ac5d-be7b21b387a6",
   "metadata": {},
   "source": [
    "# Train and validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ced27e5-cc65-4dde-b602-285ca6428a24",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c314193a-d500-464f-97fe-2e08e02c3cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e11a1ac-4eb1-4cbc-a212-6d7a49b2a6d9",
   "metadata": {},
   "source": [
    "### Build data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "733ffd5c-14b2-4a67-889f-b6679e60bd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21250 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data_generator = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.1,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1\n",
    ")\n",
    "\n",
    "train_generator = train_data_generator.flow_from_dataframe(\n",
    "    df_train,\n",
    "    x_col=\"image address\",\n",
    "    y_col=\"label\",\n",
    "    target_size=(img_height, img_width),\n",
    "    class_mode=\"categorical\",\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b86a1db5-288c-42a0-9ee5-dab3ac15d50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3750 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_data_generator = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "validation_generator = validation_data_generator.flow_from_dataframe(\n",
    "    df_validation,\n",
    "    x_col=\"image address\",\n",
    "    y_col=\"label\",\n",
    "    target_size=(img_height, img_width),\n",
    "    class_mode=\"categorical\",\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824453e8-5f7f-4a40-b9a8-e6afb373e10c",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298af813-b92d-4a7e-b419-fe477feb2210",
   "metadata": {},
   "source": [
    "### Create model archtecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86374680-aa38-4783-b38f-84c6a676d8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(128, (6,6), activation=\"relu\", input_shape=(img_width, img_height, img_channels)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(128, (4,4), activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(64, (4,4), activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128939fb-c330-45c9-a6b0-2f0ed9bf20e8",
   "metadata": {},
   "source": [
    "### Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e610a30d-53d1-4c81-b2c8-ac1bf012ce18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 123, 123, 128)     13952     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 123, 123, 128)    512       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 61, 61, 128)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 61, 61, 128)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 58, 58, 128)       262272    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 58, 58, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 29, 29, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 29, 29, 128)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 26, 26, 64)        131136    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 26, 26, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 13, 13, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 13, 13, 64)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 10816)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               5538304   \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,950,018\n",
      "Trainable params: 5,948,354\n",
      "Non-trainable params: 1,664\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db97a3f3-c4a6-4b8f-8379-a57624ae7e2e",
   "metadata": {},
   "source": [
    "### Define callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1090d76-d60c-414a-a0a0-01ad07cbc353",
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop = EarlyStopping(patience = 10)\n",
    "callbacks = [earlystop]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470e7837-3e27-4b34-8612-1b3edfac0f71",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91cc0a38-5101-4ad2-83b4-7e920e9375f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "708/708 [==============================] - 1134s 2s/step - loss: 0.7365 - accuracy: 0.6279 - val_loss: 0.8366 - val_accuracy: 0.6171\n",
      "Epoch 2/10\n",
      "708/708 [==============================] - 1083s 2s/step - loss: 0.5499 - accuracy: 0.7212 - val_loss: 0.6510 - val_accuracy: 0.6835\n",
      "Epoch 3/10\n",
      "708/708 [==============================] - 1078s 2s/step - loss: 0.4687 - accuracy: 0.7788 - val_loss: 0.5045 - val_accuracy: 0.7589\n",
      "Epoch 4/10\n",
      "708/708 [==============================] - 1096s 2s/step - loss: 0.4143 - accuracy: 0.8112 - val_loss: 0.5208 - val_accuracy: 0.7677\n",
      "Epoch 5/10\n",
      "708/708 [==============================] - 1097s 2s/step - loss: 0.3756 - accuracy: 0.8321 - val_loss: 0.3937 - val_accuracy: 0.8256\n",
      "Epoch 6/10\n",
      "708/708 [==============================] - 1113s 2s/step - loss: 0.3424 - accuracy: 0.8494 - val_loss: 0.3797 - val_accuracy: 0.8403\n",
      "Epoch 7/10\n",
      "708/708 [==============================] - 1135s 2s/step - loss: 0.3218 - accuracy: 0.8604 - val_loss: 0.4154 - val_accuracy: 0.8131\n",
      "Epoch 8/10\n",
      "708/708 [==============================] - 1124s 2s/step - loss: 0.3067 - accuracy: 0.8667 - val_loss: 0.5260 - val_accuracy: 0.8043\n",
      "Epoch 9/10\n",
      "708/708 [==============================] - 1111s 2s/step - loss: 0.2938 - accuracy: 0.8731 - val_loss: 0.4869 - val_accuracy: 0.7907\n",
      "Epoch 10/10\n",
      "708/708 [==============================] - 1111s 2s/step - loss: 0.2803 - accuracy: 0.8814 - val_loss: 0.3841 - val_accuracy: 0.8445\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator, \n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=df_validation_shape[0]//batch_size,\n",
    "    steps_per_epoch=df_train_shape[0]//batch_size,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b59514c-348e-433f-a894-ee05aecd486e",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b3a9b1c-826a-451e-9cee-49ca9b275561",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"../backend/models/cats_vs_dogs_model_2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d9d31fe-4752-48d7-b8cf-143efec9cf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../backend/models/cats_vs_dogs_model_2_summary.txt', 'w') as f:\n",
    "    with redirect_stdout(f):\n",
    "        model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-dogs-vs-cats",
   "language": "python",
   "name": "venv-dogs-vs-cats"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
